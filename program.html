
<!DOCTYPE html>
<html>
<head>
<title>MOCO2022</title>
<link rel="stylesheet" type="text/css" href="style.css">
<link rel="stylesheet" href="style_program.css">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>


<body>
  <div class="wrap">

  	<div id="sidebar-left">
    <div class="topnav" id="myTopnav">
    <a href="index.html" class="active"><img src="logo2.png"  alt="MOCO2022 Logo" width="80%" /></a>
    <a href="about.html">About</a>
    <a href="air.html">Artist-in-Residence</a>
    <a href="cfp.html">Call for Papers</a>
    <a href="attend.html">Attend</a>
    <a href="program.html">Program</a>
    <a href="contact.html">Contact</a>
    <a href="javascript:void(0);" class="icon" onclick="myFunction()">
      <i class="fa fa-bars"></i>
    </a>
  </div>
</div>

<div id="sidebar-right">
  <div class="other">
  <div class="spaceheight">
  <div class="h">
  <img src="sL.png"  alt="MOCO2022 Dancers with Robot Wings" width="100%" />
  </div>
 <div class="space">

<h2>MOCO'22 Program</h2>
<p>This program is subject to change. The conference will take place at Columbia College Chicago's Film Row Building at 1104 S Wabash Ave., with Wednesday evening Practice Works at the Dance Center at 1306 S Michigan Ave.</p>

<p>Virtual events will be communicated to authors through the Discord link, which has been sent by email to authors. Please email us at movementcomputing2022@gmail.com if you need the link. </p>

<p>A map of all relevant <a href="https://www.google.com/maps/d/u/0/edit?mid=1g3gNNd2ofcoyqa_9iBwvqIu1VbLYf4Y&usp=sharing" target="_blank"><span class="under">MOCO destinations</span></a></p>

<p>Information for getting around Chicago as well as the multitude of special events in Chicago during MOCO <a href="todo.html"><span class="under">can be found here.</span></a></p>

<ul>
  <li>(L): Long paper	20 minutes presentation + 10 minutes q+a</li>
<li>(S): Short paper	10 minutes presentation + 5 minutes q+a</li>
  <li>Workshops and Posters happen simultaneously</li>
  <li>Practice Work Presentations + Demos: 10 minutes presentation inclusive of q+a</li>
  <li>(IP): In person</li>
<li>(V): Virtual</li>
  <li>(LBW): Late-Breaking Work</li>
   </ul>

    
    
   <!-- partial:index.partial.html -->
   <!---Thursday Schedule---->
   <table>

     <tr>
       <th></th>
              <th></th>
       <th>Wednesday, June 22</th>

     </tr>

     <tr class="even">
       <td></td>
       <td>8:00-9:00am</td>
       <td>Registration</td>
     </tr>

     <tr class="odd">
       <td></td>
       <td>8:30-9:00am</td>
       <td>Welcome Remarks</td>
     </tr>

     <tr class="even">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>9:00-11:00am</td>
       <td>Workshops | Moderated by Kristin Carlson</td>

     </tr>


     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr class="even">
               <th></th>
               <th>Title</th>
               <th>Author</th>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Blockchain Feels: A Workshop in Choreographing Complexity (V)</td>
               <td>Renee Carmichael</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Blockchain Feels is a workshop that explores movement as a critical tool for understanding the technological complexity of the blockchain (a decentralized, trustless, cryptographically safe registry technology) and for developing new questions at the scale of the body. The workshop begins with the hypothesis that there are differences in the experiences of the body when exploring blockchain technologies as compared to other web technologies to produce choreographies for the blockchain that go beyond fixed uses and pre-determined binaries. Whether speculative or executed, these choreographies will come together in a custom-designed website that acts as the basis for an open archive, a starting point to think movement, and the blockchain within a constantly shifting landscape. The workshop format provides an open space to dynamically entangle practice, theory, aesthetics, and experiences to explore new languages that consider the ephemerality of the how, the incomprehensible, and the process in the way that blockchain technologies are questioned and measured today. The workshop's methodology consists of a “reading through” approach, in which theoretical debates, material explorations of blockchain algorithms, a variety of movement practices, and the emerging choreographic outputs are entangled to help understand the complexity and to propose possibilities for including the body and movement as a critical part of blockchains.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>The BESST System: Explicating a New Component of Time in Laban/Bartenieff Movement Studies Through Work With Robots (IP)</td>
               <td>Amy Laviers and Cat Maguire</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">In order to develop interactive technology that interfaces more and more with humans and their full embodiment, robotics engineers need a means for reasoning about natural and artificial movement that is rooted in human experience. Movement studies -- a broad field covering somatics and choreography -- can act as a bridge to allow engineers to create technology that interacts with human movement in order to serve societal needs. An interesting, perhaps unanticipated result of this process is that applying movement studies to robotics requires a sharpening of the methodology. Thus, the branch of movement studies initiated by Laban and Bartenieff -- sometimes described as the Laban/Bartenieff Movement Studies (LBMS) -- provides an important tool set for understanding movement patterns in context. This work is organized through four established components: Body, Effort, Shape, and Space (or the BESS System). The observation and movement-based workshop described in this extended abstract will share ideas about temporal patterns in movement, explicated as a new component, Time, establishing the BESST System. This component grows from working with machines, which must deal in user-specified and designed quantitative units of time, as a way to describe motion that is not quite as fluent as human motion. Some machines do not portray clear ideas about intent and relationship from their movement, but it is typically possible to measure the amount of time an action took and frequently phrasing is observed through stops and starts of different machine parts. Thus, while a particular example of artificial movement may not rise to the level of creating a clear dynamic quality, it does use elements collected in this component of Time, e.g., sequencing, duration, tempo, and phrasing. This workshop will offer an experiential inroad to these elements and their use inside of contemporary approaches to robotics.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
           </table>
           <table>
           </table>
       </td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>11:00am-12:00pm</td>
       <td>Poster Presentations and Coffee | Moderated by Greg Corness</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Title</th>
               <th>Author</th>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Smooth Operator: A Device for Characterising Smoothness in Body Movement</td>
               <td>Adrian Artacho and Leonhard Horstmeyer</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">In our research we faced the problem of characterising smoothness in human movement. Even though 'smooth' is a common way to describe and conceptualise movement in the performing arts as well as in informal speech, we realised the need for a tool to differentiate between various degrees and modes of smoothness. We propose that smoothness operates at different interlocking orders. These appear only in aggregation, intertwined with other qualities of body movement. Akin to how a spectrometer splits light into a spectrum of frequencies, we developed a method to measure the degree of smoothness in each order, as an epistemic tool for dance practitioners to investigate the quality of body movement from a fresh perspective. To this end we have implemented a device that provides dancers with aural, haptic and visual feedback in real time, taking into account the idiosyncrasy of the discipline of contemporary dance, and the constrains of a dance practice session.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Exploring the Design Space for Body Transformation Wearables to Support Physical Activity through Sensitising and Bodystorming</td>
               <td>Ana Tajadura-Jiménez, Judith Ley-Flores, Omar Valdiviezo, Aneesha Singh, Milagrosa Sánchez-Martín, Joaquín Díaz Durán and Elena Márquez Segura</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Negative or disturbed body perceptions are often interwoven with people’s physical inactivity. While wearables can support body perception changes (body transformation), the design space of body transformation wearables supporting physical activity remains narrow. To expand this design space, we conducted an embodied co-design workshop with users. Using conceptual and tangible sensitizing tools, we explored/reflected on bodily sensations at three moments of movement execution (before/during/after). Conceptual tools were used to evoke, reflect and capture past lived experiences, while tangible tools were used as ideation probes for sensory bodystorming. Two design concepts emerged, reflecting diverging approaches to body transformation wearables: one focused on reminders and movement correction; the other on sensory augmentation and facilitation. We reflect on how each facilitates useful representations of body sensations during movement, and present methodological recommendations for designing technology for sensory augmentation in this area. Finally, we propose a preliminary prototype based on our design concepts and discuss future steps.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>From capturing the embodied social experience to music composition: data as mediation </td>
               <td>Vilelmini Kalampratsidou, Pandelis Diamantides, Marina Stergiou, Katerina El Raheb, Yannis Ioannidis, Filia Issari, Eugenie Georgaca, Evangelia Karydi, Flora Koliouli, Dora Skali, Panagiotis Giokas, Virginia Vassilakou and Yannis Pappas</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">This work presents the first steps of a journey from capturing the embodied social experience to creating contemporary sound art through digital means. In the framework of the Transition to 8 project, residents of the Greek City of Eleusis expressed their feelings and perspectives on the social issues of their community through organized sociodrama sessions. Some of the participants wore watch-sized technology that enables the recording of various bodily data, such as heart rate, temperature, and skin conductance (affiliated with emotional arousal). The data collected by applying mixed-methods approach are organised, analysed and shared with artists to inspire them and provide them with the first material. In Transition to 8 project we aim build a platform and methodology that uses data as mediation of a local social issue and perspectives of the citizens to a global artistic community. As a proof of concept, one sound composition artistic piece has been created so far, to guide this process, while the building of the platform and further experimentation are an ongoing endeavour.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Virtual Dance Mirror: A functional Approach to Avatar Representation through Movement in Immersive VR</td>
               <td>Saliha Akbas, Asım Evren Yantac, Terry Eskenazi, Kemal Kuscu, Sinem Semsioglu, Onur Topal Sumer and Aslı Öztürk</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Immersive VR (IVR) technologies offer new possibilities for studying embodied interaction with different sets of constraints and affordances for action-taking while using one’s physical body. In this study, we designed and prototyped a VR dance experience, Virtual Dance Mirror, where a dancer’s bodily movements are reflected on a 3D avatar model using a motion-capture suit. We investigated the novel possibilities for avatar representation based on the expression of movements available for dancers in an IVR environment to augment communication between dancers and choreographers and explore strategies for storytelling. After a preliminary briefing session, we conducted a user-study with five dancers with semi-structured interviews. We then conducted another focus group study with dancers and choreographers based on these virtual dance performances. Our findings support HCI literature on virtual body design to facilitate collaboration and non-verbal communication between VR users.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Motion-Sensor Programming for Accessible Interfaces</td>
               <td>Abdelrahman Elawadly, Dexter Aichele, Eva Coulon, Nick Hager, Tina Wang, Julian Le and William Bares</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Motion-sensing devices that track the movement of human bodies, rigid body marker clusters, and eyes offer a diverse range of alternative input modalities that can be applied to create more accessible interfaces for diverse user populations. We present a set of curricular modules and open-source software suitable for use at the two- or four-year college level to teach accessible design featuring programming of
   three different types of motion-sensing input devices. The intended audience are software developers or students in computing-related fields. The curricular modules are open source and include guidelines for accessible design, readings, lab activities and project assignment prompts.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Incoherent Robot Groups: How Divergent Motions within a Robot Group Predict Functional and Social Interpretations</td>
               <td>Alexandra Bacula and Heather Knight</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Research has found homogeneous robot groups can be intimidating, but few have studied the impact of intentionally incoherent robot group motion. This work explores incoherent group motion through an online user study, varying how robots move relative to a human figure entering the scene. Online participants (N=240 participants) rated twelve research conditions across various social and functional goals. Results showed coherent groups had the strongest communication signals, but incoherent motion can cue more complex communications. Coherent motion towards was threatening and blocking, and coherent motion away was avoidant and harmless. Coherent stillness was inviting. Subgroup size linearly affected communication strength.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>Reinforcement Learning Based Dance Motion Generation</td>
               <td>Markus Toverud Ruud, Tale Hisdal Sandberg, Ulrik Johan Vedde Tranvåg, Benedikte Wallace, Seyed Mojtaba Karbasi and Jim Tørresen</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Generating genuinely creative and novel artifacts with machine learning is still a challenge in the world of computational science. Achieving a creative machine learning agent can be beneficial for applications where novel solutions are desired and may also optimize search in creative problem-solving applications. The interactive properties of Reinforcement Learning (RL) and the fact that it has not been widely used in this context earlier makes it an interesting approach. This paper shows how a Reinforcement learning-based technique, in combination with Principal Component Analysis (PCA), can be utilized for generating varying movements based on a goal picking policy. The proposed model is trained on a data set of motion capture recordings of dance improvisation. This data forms the basis for goal selection and generation of new dance sequences. Our study shows that the trained RL agent, when presented with a dance recording, can learn to pick sequences of dance poses that are coherent, resemble dance, and has more compound movement. By qualitative and quantitative measurement of the data from the simulation, we find that the RL and PCA algorithms motivate the agent to perform larger, more complex movements. This paper explores creative expression by generating varied dance movements using an RL and PCA-based approach.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>The Impacts of Virtual Reality Avatar Creation and Embodiment on Transgender and Genderqueer Individuals in Games (LBW)</td>
               <td>Zoey Reyes and Joshua Fisher</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Virtual Reality (VR) developers create experiences that result in both gender dysphoria and euphoria for Transgender and Genderqueer individuals. Avatar Creation Interfaces (ACIs) and the game mechanics associated with them engage with these effects. Since all experiences in VR begin with a body in space, ACI mechanics can inform a more inclusive experience for Transgender and Genderqueer individuals with the avatars they create and subsequently embody for extended periods of time.. Avatar Creation Interface mechanics are a foundational step toward a more inclusive experience for Transgender and Genderqueer individuals. Previous research of Avatar Creation Interfaces’ effects on people with gender dysphoria and euphoria is limited and does not address the subject in VR. However, experiences in which players inhabit a body that is not their own are common. VRChat, a community game, allows its players to choose from a vast assortment of bodies, from realistic to abstract, and allows players to upload their own avatar to the game. The Machine to be Another experience enables individuals to experience life from another’s perspective with a different body. One version had players share the perspective of a Trans Man. These experiences do not directly investigate the potential impacts that Virtual Reality might have on Transgender or Genderqueer individuals. Given this knowledge gap, research on the euphoric and dysphoric effects of embodiment that Transgender and Genderqueer individuals have within VR is constructive.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>A Collective Synthesis (LBW)</td>
               <td>Amay Kataria</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Supersynthesis is an interactive audio-visual instrument that invites people to collectively engage with an interactive expression. Accompanied with a physical installation, it utilizes the medium of light and sound to create a communal experience where the audience activates the piece and the space around it by interacting with it through an online interface. The objects, architecture, bodies, and the lights itself contribute to an experience that is rooted in movement and computing. Community is a network effect of individuals imposing themselves on their environment and vice-versa. Through communal computing, Supersynthesis sets up conditions to collectively experience the self and individually experience the collective.</td>
                   </tr>
                 </table>
               </td>
             </tr>

           </table>
           <table>

           </table>
       </td>
     </tr>

     <tr class="even">
       <td></td>
       <td>12:00-1:30pm</td>
       <td>Lunch Break</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>1:30-3:30pm</td>
       <td>Doctoral Consortium | Moderated by Umer Huzaifa and Catie Cuan</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>1:30pm</td>
               <td>Relational Social Interaction and Communication. A Performing Arts Approach to Non-Verbal Human-Robot Interaction (IP)</td>
               <td>Irene Alcubilla Troughton</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">This thesis explores social interaction and communication between humans and robots from the perspective of what the performing arts have to offer to this debate. I argue that the performing arts can provide an approach to non-verbal social interaction and communication that is more embodied and relational, and that brings to the fore unaddressed aspects in HRI.
   How to create a convincing and engaging social interaction between humans and robots is key in social robotics. Due to the expertise of the performing arts in these matters, social roboticists have turned to this field to explore different aspects of social interaction and communication (Bruce et al., 2000; Cuan et al., 2018; Hoffman et al., 2008; Jochum et al., 2016, 2017; Knight & Gray, 2012; LaViers et al., 2017; Zhou et al., 2019). In this way, roboticists are already pointing at an interesting and fruitful collaboration; however, the way in which they are approaching this intersection is not always tapping into the full potential of what theatre and dance have to offer to robotics. Most social robotics experiments are still dialogue-based (De Wit et al., 2018; Van Dijk et al., 2013), and when they make use of movement, they do so from the perspective of what kind of internal states, communicative content or social codes it can express (Anzalone et al., 2010; Breazeal et al., 2003; Lemaignan et al., 2012; Simmons et al., 2011; Syrdal et al., 2011). In this sense, I claim that most social robotic projects participate in an “interiority paradigm” with regards to movement. That is, they consider movement to be merely the expression of inner, pre-determined states, and usually just a complement to verbal interaction. This thesis will make clear how such a paradigm carries certain unaddressed assumptions about imitation, similarity, ideal imaginaries of robots and humans, as well as transparency in HRI.
   </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>1:50pm</td>
               <td>Exploring the feeling of togetherness in virtual spaces: Empathy and synchrony in human-computer interactions (IP)</td>
               <td>Julia Ayache, Guillaume Dumas, Daria J. Kuss, Darren Rhodes, Alexander Sumich and Nadja Heym</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Usually occurring during face-to-face interactions, interpersonal synchrony (SYNC) is a spontaneous tendency to synchronize speech, gestures, and physiological rhythms. However, despite its popularity for eliciting affiliative tendencies, SYNC is an ill-defined concept where goal-oriented and spontaneous SYNC are often conflated, and the mechanisms underlying the association between SYNC and social bonding remain unclear. Furthermore, recent studies identified potential detrimental effects of SYNC in human-human and human-agent interactions, requiring further investigations to identify when SYNC can backfire and prevent its misuse. This PhD thesis aims to investigate (1) the association of SYNC with different empathy components, (2) their impacts on self-other boundaries, and (3) the association of SYNC with prosocial outcomes in the context of human-computer interactions.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:10pm</td>
               <td>Measuring synchronization between movement and music toward a multi-scale approach (IP)</td>
               <td>Bayd Hamza, Guyot Patrice, Bardy Benoit and Slangen Pierre</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Current researches has demonstrated that music is not only an acoustical experience but also engages the whole human body. Due to various technological progresses such as musical rhyme estimation and motion capture systems, the evaluation of synchronization performances between a large scope of movements and musical rhythms at multiple temporal scales, and across multiple individuals in today possible. My doctoral thesis focuses on the analysis of multi-scale synchronization during movement and music. It is based on data produced from motion capture of individuals and groups in our laboratory as well as in more natural settings, and on sound synthesis of multi-scale rhythmic content. These data are analyzed using different approaches from Artificial Intelligence, including recurrent neural networks and probabilistic graphical models. The objective is to build architectures and applications that facilitate the human evaluation of synchronization as interactive, corporeal, and in real-time. In my PhD we defend the view that modeling multi-scale parameters of musical rhythms and motion involves music rhythm at different scale, such as Beat or Tatum and micro-timing, coupled to body joint angles or temporal displacement of key parts of the body. To reach this goal, I propose with my supervisors to develop algorithms and methods based on computer vision and artificial intelligence (AI), to analyze multi-scale temporal parameters of music and movement. Next, we will focus on our ability to synchronize with each other, using mathematical models and different applications.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:30pm</td>
               <td>Do Positive Emotions Strengthen the Spontaneous Interpersonal Movement Synchronization? (IP)</td>
               <td>Andrii Smykovskyi, Stefan Janaqi, Marta Bieńkiewicz and Benoît Bardy</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">How do emotions shape human interactions? The effect of naturalistically induced emotions on interpersonal group synchronization where N>2 is not known (Fujiwara & Daibo, 2018; Hove & Risen, 2009). My thesis work focuses in particular on the decoding of the embodiment of agents’ emotions in the interactional dynamics and the impact of valence and arousal of agents’ emotions on synchronization performance. I do not study movement data in isolation, but enrich it with the data from psychological and physiological modalities as well, which allow for multi-scale approaches for data analysis.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:50pm</td>
               <td>!unsentimental (V)</td>
               <td>Kwame Kyei-Boateng</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Through the lens of identity, !unsentimental integrates physical performance and computational arts, while probing the use of 3D visualization as a means of expressing emotional states. The performance piece explores the changing nature of technology, the stage, and the performer, and the shifting boundaries and invisible connection between the virtual and the real. In !unsentimental, the virtual world becomes an extension of the physical body.</td>
                   </tr>
                 </table>
               </td>
             </tr>
                    <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>3:10pm</td>
               <td>Sound, Motion, and the Brain: how music and sounds affect the creation of movements within an improvised performance)</td>
               <td>Ioannis Sidiropoulos</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">The interdisciplinary doctoral project combines performing arts with cognitive neuroscience involving functional magnetic resonance imaging (fMRI) of participants to explore perception-action coupling for understanding creativity in the context of responsive movement improvisation.

It questions how recorded contemporary music and environmental sounds influence the performer in the creation and execution of movements. Information gathered through the research will be used to inform the creation of an experimental performance. Thus, the experimental process will map and measure the physical and mental responses of performers (dancers and actors) while improvising to the recorded sounds both in the studio and whilst in an MRI scanner.

Analysis of the data for similarities will, it is hypothesised, lead to a better understanding of the perceptual and cognitive mechanisms at play in how performers interpret and respond to music. This process will be used to further develop experimental performance processes in dance and physical theatre, enhancing choreographic and performer know-how of the ways that different music and sounds influence movement responses. It aims to propose a creative process based on pragmatic information, intending to reduce artistic bias while creating new performance work.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>

           </table>



           <tr class="even">
             <td></td>
             <td>3:30-4:00pm</td>
             <td>Coffee Break</td>
           </tr>

           <tr class="odd">
             <td></td>
             <td>4:00-5:00pm</td>
             <td>Keynote Presentation: Norah Zuniga-Shaw <i>Livable Futures</i></td>
           </tr>

           <tr class="even">
             <td></td>
             <td>Floating Time: Grab dinner and see practice works when it suits you best</td>
             <td>Dinner Break</td>
           </tr>


             <tr class="odd">
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>5:30-9:00pm</td>
               <td>Practice Works: Performances and Installations | at the Dance Center 1306 S Michigan Ave. | Moderated by Greg Corness and Susan Imus</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=3>
                   <table>
                     <tr>
                       <th></th>
                       <th>Time</th>
                       <th>Title</th>
                       <th>Author</th>
                     </tr>
                     <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>5:30pm</td>
                       <td>Outside Street Performance on Michigan Avenue: InVisible (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">InVISIBLE is a mixed-reality performance that examines the connection between bodies that are immersed in two different realities - Virtual Reality (VR) and Real Life (RL). One performer is immersed in VR; His eyes are covered with an Oculus Quest headset. All he can see is a white empty virtual grid. Another performer is immersed in RL. She sees the street, the architecture, the natural elements in the environment, and the people who pass by. She holds the controllers of the headset that the VR performer is wearing. The performance starts with the two performers standing in front of one another. Neither of them has access to the other's view. However, once the RL performer moves her hands, a graphical mark appears inside the virtual world. The VR performer notices the movement's traces and uses them to navigate his way in the physical reality. In a way, this physical-digital path is similar to the wind - it wraps itself around the VR performer and provides clues for navigation. While the RL performer uses the controllers to mark a safe path for the VR to follow, she also traces RL objects and bodies to manifest them inside the virtual world. The VR performer responds to the visual cues by embodying the drawings, translating them into movement, following them, and figuring out what the body can do in this new world that is being created in front of his eyes.
                             </td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                       <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00pm</td>
                       <td>Room 102 Dance Center: Floating Departures: Developing Quarantine Dance Technique as an Artistic Practice Beyond the Pandemic (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">We describe our process of quarantine dance technique in making the dance film and meditation, Floating Departures. This work, created during lockdown in 2021, brings together dance movement, poetry, painterly styles, and sound to explore cyclical patterns and points of departure in movement and life. To create Floating Departures we used a broad range of technologies–from everyday objects to smartphones to AI art systems. We experiment with various techniques to record ourselves and bring our movement together in a shared digital space with post-production video editing techniques. Using a bricolage approach, we incorporate materials, such as bubble wrap and balloons, to transform our spaces and explore our personal experiences during lockdown. We construct multiple layers of reality that are further transformed in unanticipated directions with AI technologies. Through our creation process, we develop a collective physical body to explore a new realm, unbound by reason or logic, that was only made possible through our remote collaborative processes and technologically-mediated interactions.
                             </td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     
                        <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00pm</td>
                       <td>Room 102 Dance Center: Can I Present You With A Touch Just Made? : Merging social gathering and artistic research through new forms of digital live art (V)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Kate Stevenson, Alys Longley and Jeffrey Holdaway have been developing the digital exhibition a tilting body of precarious maps and migrant constellations since late 2020. The work began as a series of artist-maps created for Longley’s collaborative exhibition Let us Drink the New Wine Together!/ Beberemos El Vino Nuevo Juntos! exploring ways for artists to dwell and rewrite our shared world together through 2020 - 2022, in the form of artist maps and postal art projects. Stevenson extended this exhibition into virtual terrain through her experience with digital art and online platform creation, with sound and video artist Jeffrey Holdaway translating visual art, video, sound and performance strategies into new formats for this context. In the absence of traditional forms of gallery and theatrical exhibition, we have collaborated with over sixty artists from all continents of the world to translate a series of artist maps and postal artworks into a labyrinthine series of interactive, three dimensional, sonic and relational worlds.
                             </td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     
                         <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00-7:30pm</td>
                       <td>Room 200 Dance Center: danceON and softWEAR: education-level creative coding and programmable wearables (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Our research collaborative has been exploring movement computing educational technology experiences. That is, we have been building tools that simultaneously support both movement and computing learning objectives at entry-level. We will demo two products in development. danceON is a domain-specific language and a web app that allows users to create interactive graphics overlaid on video from pre-recorded or live (webcam) sources. softWEAR is a solderless and breadboardless ecosystem utilizing sensors, LEDs and the Adafruit Trinket M0 (soon to be micro:bit) and is designed to create a workflow from ideation, prototyping and iteration to durable, wearable final project embedded into clothing or accessories.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     
                            <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>7:30-9:00pm</td>
                       <td>Room 200 Dance Center: And She Will Sound The Alarm: Performance and Demo of the Body Sample Player (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Kinesthetic empathy, in the context of kinesthetic interaction, is defined as the ability to encode and decode the input of other users of the system or to sense a shift in the system itself. Achieving this between two players in a virtual performance system involving sound is difficult to achieve, and requires that players have an existing connection with the system that marries the sonic and kinesthetic sense known as “auditory kinesthesia”. These sensibilities can be facilitated through the design of the system itself following the principles of usability and kinesthetic interaction, or through performer training. In traditional music and dance, kinesthetic empathy exists both between players and between players and audience. It is much more difficult to achieve this in the context of interactive performance as the audience does not have the same culturally specific empathetic response when observing digitally mediated performance as they do when viewing forms of music and dance they are already familiar with. The Body Sample Player is a digital musical instrument and performance platform that uses real-time data gathered by body tracking cameras to control the volumes of looping sound samples, turning the body itself into a sound controller. It is our contention that by giving audience members first-hand experience engaging with this instrument before witnessing a performance, they will more easily be able to recognize expressivity in the performance, and in doing so, will experience a higher level of kinesthetic empathy. Therefore, we propose a performance of And She Will Sound the Alarm, a piece created for the Body Sample Player system, preceded by a demonstration in which conference attendees can interact with the system. We will use the demonstration as a means of measuring the level of entrainment when observing a short performance both before and after having first hand experience with the system. We will then use a general survey to see whether this first hand experience increases the sense of expressivity during the full performance.
                             </td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     
                          <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00-7:30pm</td>
                       <td>Room 210 Dance Center: Desquamation: An Interactive 360-Video & Choreographic Study on Socio-cultural Voyeurism, Agency, & Race (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Desquamation is a three-part choreographic work that challenges viewer experience of socio-cultural identity theories through an interactive, 360 degree-video lens. This work seeks to explore the impact of interactive three-hundred-and-sixty (360) degree- video through dance and socio-cultural theory. In this work, the power dynamics of the traditional performer-to-audience relationship shifts, introducing a new layer of interaction into the [virtual] performance space. Traditionally, audience members viewing dance-films are asked to be passive in their engagement with the work. In this exploration, project collaborators seek to transform the viewer's experience as an audience member from static to active. The objective of the selected performance medium serves to provide audience members agency, by being in control of their own gaze, in their experience and interpretation of the work. Audience member experiences vary upon a number of factors including (but not limited to): order of videos viewed, location of dancer on-screen, setting, choreographic and/or sonic meaning-making, and/or lived experience. Desquamation aims to exhibit the interconnectedness of socio-cultural voyeurism, dance, and race through a fresh, interactive lens of 360° video technology. The interdisciplinary collaborations explored in this work demonstrate the use of technology, choreographic and cultural movement practices, and theoretical and philosophical perspectives on movement that aim to diversify the spectrum of movement computing and technology in the future.
                             </td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     
                 
                     
                     <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>7:30-9:00pm</td>
                       <td>Room 210 Dance Center: Recessed (V)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Recessed explores the emotional impact of users’ engagement with the web browser through a series of movement-driven vignettes. Pop-up windows choreographed to appear in specific configurations capture the intimacy of bodies moving within a virtual space. The site, a custom-coded movement palimpsest, layers recorded performances to highlight diverse narratives in motion.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00-7:30pm</td>
                       <td>Room 202 Dance Center: Manus Tremens: A structured improvisation for accelerometer-controlled vibration motors, amplified toy harp, and live sound processing (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Manus Tremens is a structured improvisation for accelerometer-controlled vibration motors, amplified toy harp, and live sound processing. The performer uses two vibration motors to directly actuate the strings of a toy harp. Accelerometers affixed to the performer’s wrists directly affect the vibration intensity of each motor, which enables nuanced dynamic control of the sounds being produced. This design of this system and performance are inspired by the cimbalom, an Eastern European hammered dulcimer instrument. The Manus Tremens system also represents a collaborative approach to sound-making that combines human movements and mechatronic sound actuation.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
              
                              <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>7:30-9:00pm</td>
                       <td>Room 202 Dance Center: Dance VR Adventure Games (LBW) (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">This is a demonstration of two work-in-progress Virtual Reality dance games: "VR Dance Adventure" and "Master Dancer." Each has two primary goals. VR Dance Adventure's are to 1) to get the user to pay close attention to dance movement, and 2) to determine if there might really be communicative content in an “abstract” contemporary dance. "Master Dancer" seeks 1) to instruct users in some basic movement energies and then to get them to use those new skills to move in their own creative way, and 2) to be exposed to some historical dance information. After each demonstration there will be a feedback session to get suggestions on further development, or if need be, to pivot in a new direction.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                   
               
                 

                   
                     <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00-7:30pm</td>
                       <td>Room 300 Dance Center: Pendular (IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">We are naturally drawn to observing movements such as a balloon flying into the sky, or a pendulum swinging in perfect time. These motions give us a glimpse into the infinite interconnected variables that make up our reality. They captivate us in their seeming randomness (when there are too many variables to model) or their predictability (when the system can be accurately represented with mathematical functions). Observing such phenomena provides us with a moment of meditative peace and contemplation, not unlike the feeling you get when listening to music you love. In our interactive musical system Pendular, we draw a connection between these two sensibilities. The work introduces a method of translating the invisible forces around us into the sonic realm, in an effort to inspire imagination and memory through movement and sound.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                
                        <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>7:30-9:00pm</td>
                       <td>Room 300 Dance Center: KNN FM v.3: Exploring social dynamics through the interactive sound environment (LBW)(IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">The proposed demo-workshop is a part of research I am conducting for my MFA thesis project. Conceptual background of the latter is defined by critical re-thinking of the idea of digital interface and its role in structuring the social reality of capitalist computerized production. In this regard, project KNN FM aimed at exploration of collective body awareness in a virtual space realized as an interactive sound environment.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
            
                     
                       <tr>
                       <td id="collapseButton" onclick="collapse(this)">+</td>
                       <td>6:00-7:30pm</td>
                       <td>Room 103b Dance Center: Bread Symphony: A Cross-Species Collaboration for Spiritual and Material Nourishment (LBW)(IP)</td>
                       <td></td>
                     </tr>
                     <tr id="hidden">
                       <td></td>
                       <td colspan=2>
                         <table>
                           <tr>
                             <th>Abstract</th>
                           </tr>
                           <tr>
                             <td class="abstract">Abstract	A group of four human collaborators would be delighted to improvise, alongside our nonhuman collaborators - yeast and bacteria - the Bread Symphony: A Cross-Species Collaboration for Spiritual and Material Nourishment. In Bread Symphony the live fermentation of bread starter is sonified using Co2, temperature, and proximity sensors and mapped to sound in Max MSP. The Symphony relies on live fermentation data input and therefore authors itself live. Analog sounds of the CO2 bubbles popping will be amplified with microphones as another sonic layer. While the nonhuman actors are generating one soundscape, the human bread-makers will be mixing, kneading, and shaping the dough. The movement of our hands and bodies will be translated (gyroscope and pressure sensors to MaxMSP) to an overlapping layer of improvised sound.</td>
                           </tr>
                         </table>
                       </td>
                     </tr>
                     

                   </table>
                       </table>
                       <p></p>
                       <p></p>
                       <p></p>
       </td>
     </tr>
   </table>

   <!---Thursday Schedule---->

   <table>

     <tr>
       <th></th>
              <th></th>
       <th>Thursday, June 23</th>

     </tr>

     <tr class="even">
       <td></td>
       <td>8:00-9:00am</td>
       <td>Registration</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>8:30-10:30am</td>
       <td>Paper Session #1: Design Analysis and Visualization in Movement | Moderated by Luke Dahl</td>
     </tr>

     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>8:30am</td>
               <td>Performers' Use of Space and Body in Movement Interaction with A Movement-based Digital Musical Instrument (L)</td>
               <td>Doga Cavdir and Sofia Dahl</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Movement-based musical interfaces support performers' music and movement expressions by drawing from expertise and creative practices of both disciplines. In this work, we qualitatively and quantitatively analyze the movement interaction of participants with Bodyharp, a movement-based musical instrument. This wearable hybrid instrument offers musical affordances that allow performers to extend beyond small gestural spaces. Its wearable design encourages the performers to move while creating music and to express while using their bodies.
   Data was collected from twenty participants' interactions, reflections, and compositions with Bodyharp. Video recordings of the experiment were annotated and qualitatively analyzed to reveal which performed gestures directly contribute to sound production and modification and which gestures accompany these musical actions. Musical Gestures Toolbox was used to further quantify the gestures. Using the Laban Movement Analysis framework, we observed participants' use of space and body in their interaction with a movement-based musical instrument and how their backgrounds in music or movement (based on participants' self-reported experiences) influenced the interaction.
   Our results offer design practices for creating new interactions at the intersection of music and dance.</td>

                   </tr>
                 </table>
               </td>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>9:00am</td>
               <td>Stage Together: Remote Rehearsal of Theater Blocking (S)</td>
               <td>	Stephanie Fulton, Dexter Aichele, Eva Coulon, Anna Conser, Matthew Kizer and William Bares</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">The COVID-19 pandemic has forced many theater productions to shift to remote
   rehearsals and performances using online video conference platforms. Video conference software makes it possible to share individual performances, but loses the sense of presence together and provides no way to visualize blocking movements on the shared space of the stage. Rehearsal by video conference can cause productions to either omit blocking altogether or have only a few on stage rehearsals, sometimes just days before opening. This paper presents a Web-based system that enables productions to remotely rehearse blocking at any stage of the production. Remote performers can propose their own blocking moves, rehearse scripted blocking cues, and visualize blocking moves in a shared virtual stage. This paper describes the design, implementation, and focus group evaluations of Stage Together with a college theater production of Medea by Euripides.</td>

   </tr>
   </table>
   </td>
   </tr>


             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>9:15am</td>
               <td>Generative Dance - a Taxonomy and Overview (L)</td>
               <td>Daniel Bisig</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Generative Art is a creative approach that has found applications in several artistic disciplines. In some of these disciplines, formalization has historically played an important role, which predisposes them for employing generative methods. In dance, the relationship to Generative Art is less obvious and the role of formalization is more contested than in other disciplines. This paper tries to contribute to an understanding of the specific role that Generative Art currently plays in dance.
   It does so by proposing a taxonomy of topics that cover both common and dance specific aspects of Generative Art. This taxonomy is used for comparing a wide diversity of generative works that have been created in the context of dance.</td>

   </tr>
   </table>
   </td>
   </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>9:45am</td>
               <td>Radiant Soma: Visualization of Movement Through Motion Capture and Lasers (S)</td>
               <td>Eugenia S. Kim, Jayson Haebich and Alvaro Cassinelli</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">The human body has historically been a constant source of fascination in the arts and sciences. With the advent of digital computing technology, the debate over the virtues and disadvantages of physical versus virtual bodies has increased exponentially. One of the greatest challenges is translating the “essence” of human movement into the digital realm from physical reality. Radiant Soma emphasizes the ephemerality of human movement by visualizing motion capture data with lasers within the context of Korean shamanism and Stone Tape theory. The installation of light and phosphorescence transforms inanimate objects into metaphorical shamans emitting a constant stream of lively spirits.</td>

                   </tr>
                 </table>
               </td>
             </tr>



             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>10:00am</td>
               <td>Exploring costume-avatar interaction in digital dance experiences (S)</td>
               <td>Marina Stergiou and Spyros Vosinakis</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Dance in the digital world is an area that has been explored vividly the last years with applications in education/learning, cultural heritage preservation, archiving and performance. Digital costume is a concept complimentary to dance that also evolves as technology grows, with virtual fashion shows, digital-only clothing, virtual showrooms, and recreation of historic costumes to make their appearance more and more often. However, their intersection hasn’t been quite explored yet although it has a lot to offer in areas like digital fashion design, animation, games and artistic performances. We envisage that combining realistically simulated clothes along with motion data can add a significant potential to movement research. Specifically for learning or exploring movement in digital environments, the representation of the dancer (3d avatar) is an important aspect to be examined, for example, specific digital clothes might enhance movement understanding and thus affect learning, in some cases. Having the hypothesis that different avatar and/or cloth visualizations might lead to different results in movement, an experiment with users has been conducted in an immersive Virtual Reality environment. We setup a testbed environment, where avatars of different forms (abstract, anthropomorphic, robot) and with different costumes could perform dances. The users were immersed in the environment and had to repeatedly follow the dance steps with all avatar-costume combinations. Their performance was then rated by experts regarding the general quality of movement, the rhythm, and the steps. The analysis of these results provided insights on the effect of costumes and avatar types on the above parameters and along with qualitative aspects, wishes to initiate a study around the limitations and opportunities of combining costume and dance in digital, interactive environments.
                     </td>

                   </tr>
                 </table>
               </td>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>10:15am</td>
               <td>Towards A Framework for Dancing Beyond Demonstration (S)</td>
               <td>Patrick Martin, Kate Sicchio, Charles Dietzel and Alicia Olivo</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">This paper presents a prototype framework for developing real-time human-robot performances and discusses its application within a recent choreographic work for the stage. The framework allows dancers to teach new motions to robots, while also giving choreographers the ability to compose performances from pre-built and learned behaviors. To achieve this capability, we combine behavior-based robotics and learning from demonstration approaches to construct behaviors and compose them into a performance. Our learning algorithm, dancing-from-demonstration (DFD), allows dancers and choreographers to teach new phrases to the robot and specify choreographic motifs as needed for the performance. This collaborative work culminated in a human-robot duet, where the robot incorporates a new, learned motion into its choreography within live performance. These new capabilities create a baseline for choreographers and dancers to eventually compose and perform in more dynamic, reactive choreo-robotic performances.</td>

                   </tr>
                 </table>
               </td>
             </tr>

   </table>
   <table>
   </table>
   </td>
   </tr>



     <tr class="even">
       <td></td>
       <td>10:30-11:00am</td>
       <td>Coffee Break</td>
     </tr>

     <tr class="odd">
       <td></td>
       <td>11:00am-12:00pm</td>
       <td>Artist in Residence Presentation | Yanira Castro <i>Fumblings in the Dark</i></td>
     </tr>

     <tr class="even">
       <td></td>
       <td>12:00-1:30pm</td>
       <td>Lunch Break</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>1:30-3:30pm</td>
       <td>Paper Session #2: Modelling and Experiencing Robotic Interactions | Moderated by Jessica Rajko</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>1:30pm</td>
               <td>RoboGroove: Creating Fluid Motion for Dancing Robotic Arms (L)</td>
               <td>Amit Rogel, Richard Savery, Ning Yang and Gil Weinberg</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Robotic motion has been studied for many purposes, such as effective fast movements, communicative gestures, and obstacle avoidance. Through this study, we are able to improve the perceived expressivity of a robot performing a task by generating trajectories. So far, robots have been very rigid in their movements, making them feel more robotic and less human. The concept of follow through, and smooth movement, can be used to increase animacy for a robot which leads to a more lifelike performance. In this paper, we describe the use of follow through to create dances and grooves that can match the elegance of human dancers. We created two techniques using a non-humanoid robotic arm, to simulate this for a robotic dancer. Our first technique uses forward kinematics with trajectories that are generated based on a dancer moving to a beat. We mapped various movements of a human dancer to a set of joints on a robotic arm to generate the dancing trajectory. This technique allows a robot to dance in real-time with a human dancer, and also create its own smooth trajectory. The time delay that each human body part uses is implemented as time delay in the robot movements. The second method uses impedance control with varied damping parameters to create follow through.Robotic joints with low damping can passively move in response to other movements in a robot. The arm had high damping for any active moving joints, while the rest of the arm would passively react to this excitation; similar to how a human body responds to our own movement. These two methods were compared in a study where the robot would dance to a beat and users would qualitatively and quantitatively rate the robotic movement. The results of the survey showed that both methods provided an increase in animacy and anthropomorphism of a robot dancing to a beat. Impedance control had the highest rating for animacy, anthropomorphism, full body, and individual body rating.The use of impedance can provide a simple way for a robot to dance like a human, without a change to the primary trajectory.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:00pm</td>
               <td>Professor Plucky -- Expressive Body Movement in Human-Robot musical ensembles (L)</td>
               <td>Michael Krzyzaniak and Laura Bishop</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">When people play music together, they move their bodies, and that movement plays an important role in the activity of group music making. In contrast, when robots play music with people, the robots are usually stiff and mechanical in their movement. In general, it is not well understood how the movement of such robots affects how people interact with them, or how the robot movement should be designed in order to promote certain features of interaction. As an initial exploration into these questions, we built a prototype guitar plucking robot that plucks the strings with either a) kinematic plucking mechanisms that are designed to have visually appealing movement, or b) control plucking mechanisms that do not visually move. In a pilot study we found that when guitarists play with the robot, they move their hands more and look at the robot more when it uses the kinematic mechanisms as opposed to the control ones. However, they do not report preferring the kinematic mechanisms. These preliminary findings suggest some very clear hypotheses for future followup studies.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:30pm</td>
               <td>Dance and Movement-Led Research for Designing and Evaluating Wearable Human-Computer Interfaces (L)</td>
               <td>Robin Otterbein, Elizabeth Jochum, Daniel Overholt, Shaoping Bai and Alex Dalsgaard</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Movement research and practice in the context of wearable technologies and human-computer interaction (HCI) shifts the design paradigm to the lived body. Human movement is characterized by sense, intention and expressiveness. Designing HCI from this standpoint opens up new possibilities to make computational devices and applications more accessible and integrated. This work presents an iterative, collaborative, and cross-disciplinary approach using wearable sensor bands in an open-ended performative exploration in exchange with a professional dancer. The goal is to understand the benefits and challenges of using movement-centered tools originating from dance practice and movement research and how they might feed back into the design, development and evaluation process of embodied technologies to improve human-computer interactions. Movement analysis systems and motion computation models are reviewed and leveraged in an interactive audiovisual system, with focus on using force-sensing resistors for low-level motion descriptors and Laban Movement Analysis for higher-level movement features. The artistic methodology, which combines practice and research, results, discussion of the iterative and collaborative process, and the final system architecture are the main topics presented in the paper.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>3:00pm</td>
               <td>A Dance Practice System that Shows What You Would Look Like if You Could Master the Dance (L)</td>
               <td>Shuhei Tsuchida, Haomin Mao, Hideaki Okamoto, Yuma Suzuki, Rintaro Kanada, Takayuki Hori, Tsutomu Terada and Masahiko Tsukamoto</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">This study proposes a dance practice system allowing users to learn dancing by watching videos in which they have mastered the movements of a professional dancer. Video self-modeling, which encourages learners to improve their behavior by watching videos of exemplary behavior by themselves, effectively teaches movement skills. However, creating an ideal dance movement video is time-consuming and tedious for learners. To solve this problem, we utilize a video generation technique based on deepfake to automatically generate a video of the learners dancing the same movement as the dancer in the reference video. We conducted a user study with 20 participants to verify whether the deepfake video effectively teaches dance movements. The results showed no significant difference between the groups learning with the original and deepfake videos. In addition, the group using the deepfake video had significantly lower self-efficacy. Based on these experimental results, we discussed the design implications of the system using the deepfake video to support learning dance movements.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>

           </table>
           <table>

           </table>
       </td>
     </tr>

     <tr class="even">
       <td></td>
       <td>3:30-4:00pm</td>
       <td>Coffee Break</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>4:00-5:30pm</td>
       <td>Practice Work Presentations | Moderated by John Toenjes</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:00pm</td>
               <td>Skin Hunger: A Telematic Installation (V)</td>
               <td>Courtney Brown, Ira Greenberg and Brent Brimhall</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Skin Hunger is a web-based interactive system for telematic installation and performance that plays on the zoom-style video-chat that has become ubiquitous during the COVID-19 pandemic. Participants in the telematic installation can reach across the webcam screens to virtually ‘touch’ one another. By touching or moving together, participants create virtual organisms and sounds that emerge and evolve from participant relation and interaction, making the intangible connection tangible and giving it life. Skin Hunger explores a new way of enabling embodied joint music making and movement over a distance.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:10pm</td>
               <td>Extending the Body (V)</td>
               <td>Tara Burns</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Extending the Body utilizes embodied dance practice with Virtual Reality as a partner. It produces an aesthetic response and choreographic result in both physical and virtual performance. Situated in the affective response of the body, the performer instigates a call and response between themselves and the virtual reality painting program, Tilt Brush that can be viewed from the physical or virtual world. When viewed in the physical world, additional elements of prosthetic Othering are witnessed, argued to present as “theatrical prosthesis” and cyborg feminism. The embodied practice investigating technologies response in the body has been researched for three years in proscenium, installation, multi-dancer rehearsal processes, and research presentations that delve into the auto ethnographic, phenomenological, and performative drawing lineage of the practice. More investigation is needed to understand the ramifications of this practice, but one element is clear: working with VR as a partner requires the same empathetic response as human collaboration.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:20pm</td>
               <td>Haptic Technology and Motion Capture to make dance more accessible for the Blind and Visually Impaired. (V)</td>
               <td>Max Percy, Gabriele Gabija Raudonaityte, and Mingke Wang (王明可)</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">In this practice work, the dancer’s progression in space from a still, neutral body to a body in motion, triggers motors inside a wearable haptic glove worn by an audience member. Each of the dancer’s four limbs (right arm, left arm, right leg, left leg), monitored by a Kinect camera, correspond to one of four motors embedded within the glove. Data from the performer's tracked body in motion is transmitted wirelessly to the palm and fingers of an audience member's hand, triggers the motor, and is felt as a series of touch sensations inside the glove. The greater the distance from the dancer's limb to their heart centre and the neutral body, the more vigorously the motor will vibrate. For the audience member, this movement information will trigger neurological information from the nerves of the hand to the tactile centres of the brain. Here the impulses can be translated such that an understanding of the dancer’s movements in space unfold for the non-sighted audience member.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:30pm</td>
               <td>Designing a Choreographic Interface During COVID-19 (V)</td>
               <td>Lins Derry, Jordan Kruguer, Maximilian Mueller and Jeffrey Schnapp</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">In 2019, metaLAB (at) Harvard began work on Curatorial A(i)gents, a digital exhibition that was slated to premiere at the Harvard Art Museums’ Lightbox Gallery in 2020. Half of the projects would be interactive, using mouse and keyboard conventions. With the advent of Covid-19 and the postponement of the show, the authors set out to develop an interface solution that would enable visitors to interact with the works without having to touch any public devices like a tablet. Toward this end, we prototyped a “choreographic interface” that uses machine vision and machine learning to interpret a full-torso gestural vocabulary, which is then translated into interactions. To make the choreographic interface, we relied on open-source solutions, which have all come with equal limitations and opportunities. In 2022, Curatorial A(i)gents was presented in the Lightbox Gallery, where we had the opportunity to test and demonstrate the interface. This paper discusses our design journey in making a choreographic interface using open-source technologies during Covid-19.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:40pm</td>
               <td>LocoMotion: Live Coding 3D Movement on the Web</td>
               <td>Kate Sicchio, David Ogborn, Shaden Ahmed, Ashmeet Dhaliwal, Milica Hinic, Esther Kim, Saiara Mashiat, Vic Wojciechowska</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Live coding is a practice of real-time computer programming that often is found in performance works that involve close feedback loops with performers, improvisation and on-the-fly decision making about algorithmic processes. It is found in a variety of media including electronic music, visuals and within dance and choreography. This project aims to design, implement, and evaluate a new programming language, LocoMotion, for the live coding of dance and movement through the use of “virtual dancers”, or 3D avatars in a web-based environment. With LocoMotion, choreography will be able to further engage with algorithms and networked collaboration, presented through the practice of live coding. This paper will discuss some starting points, background research, preliminary notations, initial implementation and possible applications.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:50pm</td>
               <td>“Embodying” AI: An Exploration of Expectation Through Movement Ideation (LBW)(V)</td>
               <td>Benedikte Wallace, Charles P. Martin, Clarice Hilton and Rebecca Fiebrink</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">What expectations exist in the minds of movement practitioners when interacting with a generative machine learning model?
   This paper presents the findings of a workshop wherein dancers experiment with embodying a generative deep learning model trained on dance improvisation.
   The workshop is implemented with the aims of better understanding uses for generative movement models in dancers' creative practice, and of facilitating the development of improved generative and interactive machine learning models for movement.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>5:00pm</td>
               <td>“If we try to put reality into words it falls apart; instead reality dwells in shapes”: Anne Bogart & Ellen Lauren in SITI Company’s Room (2000) (LBW)(IP)</td>
               <td>Melissa Johnson</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">In this presentation I will discuss the diagrams and gestural drawings I have created which allow me to evaluate and understand how Anne Bogart and Ellen Lauren engaged with the writings of Virginia Woolf in SITI Company’s play Room (2000) to create “a world where the body is as eloquent and articulate as the text.” (1) SITI Company’s Viewpoints practice helps to establish the rhythm of a body (or collective bodies) onstage before a script is brought in. Bogart and Lauren read Woolf’s writing deeply as they developed the play, but also studied photographs of Woolf so Lauren could learn the writer’s body as one learns “music or lines of dialogue.” (2) The “tactile, kinetic reality” of Woolf’s gestures allowed Lauren to build “physical passages” to create a “grammar…to meet the words.” My diagrams and drawings, which map Lauren’s movements onstage, have allowed me to discover a parallel narrative of movement that connects to and interacts with the script of Room as well as some of Woolf’s other writing. I suggest Lauren’s movements articulate Woolf’s observation in “A Room of One’s Own” that words are “inadequate to convey the reality of the world. If we try to put reality into words it falls apart; instead reality dwells in ‘shapes.’” (3) I want to use this paper to consider more deeply how words and movement can be measured and how my visualizations demonstrate how affective, embodied knowledge can achieve equilibrium with the intellectual knowledge of the text.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>          <tr>
                         <td id="collapseButton" onclick="collapse(this)">+</td>
                         <td>5:10pm</td>
                         <td>Collaboration in a virtual reality artwork: the experience of co-presence, co-creation and letting go (LBW)(V)</td>
                         <td>Julien Lomet, Ronan Gaugne, Cédric Plessiet, Joël Laurent and Valérie Gouranton</td>
                       </tr>
                       <tr id="hidden">
                         <td></td>
                         <td colspan=2>
                           <table>
                             <tr>
                               <th>Abstract</th>
                             </tr>
                             <tr>
                               <td class="abstract">Collaborative virtual spaces allow to establish rich relations between involved actors in an artistic perspective. Our work contributes to the design and study of artistic collaborative virtual environments through the presentation of immersive and interactive digital artwork installation, named in this paper “CH”. In this project, we explore different levels of collaborations proposed by the “CH” environment, and we question the experience of several actors immersed in this environment.
   The first level of collaboration is related to the co-presence of actors who share the same virtual space at the same time. The second level relies on a co-creative phase of a live virtual artwork between the artist, the performers and the spectators all connected in real-time to the collaborative environment of "CH". The last level corresponds to the access to an emotional state of letting go experienced by the actors engaged in a shared session of “CH”.
   The collaborative artwork “CH” was designed within a multidisciplinary team of artists, researchers and computer scientists from different laboratories. The “CH” experience is based on a live performance that involves a dancer, a musician, and collaborative spectators who are engaged to the experience through full-body movements. The performance is structured around three phases, one introductive meditative phase to discover the universe of “CH”, one active phase of co-creation of a shared landscape whose aesthetic is inspired by the German Romantism painting from 19th century, and a last phase of contemplation of the co-created landscape. In order to foster co-presence, each participant of the experience is associated to an avatar that aims to represent both its body and movements. The music is an original composition designed to develop a peaceful and meditative ambiance to the universe of “CH”.
   One originality of the “CH” experience is the exploration of spectators’ “letting go”. We propose to try to define and characterize this complex state of emotion and to present how the “CH” experience intend to conduct the spectators to reach it. Through the analysis of this experience, we propose a model of letting go resulting from immersion and interactivity, considering the collaboration, curiosity of creation, involvement of the spectator in the interactive process and contemplation of the resulting virtual environment.
                               </td>
                             </tr>
                           </table>
                         </td>
                       </tr>
 <tr>
                         <td id="collapseButton" onclick="collapse(this)">+</td>
                         <td>5:20pm</td>
                         <td>Q+A Session</td>
                         <td></td>
                       </tr>


           </table>



           <tr class="even">
             <td></td>
             <td>5:30-7:30pm</td>
             <td>Dinner Break</td>
           </tr>

           <tr class="odd">
             <td></td>
             <td>7:30pm</td>
             <td>Art on the Mart Social Event + Riverwalk</td>
           </tr>


                   </table>
   <p></p>
                   <!--Day 2-->


   <!---END OF EDITABLE CODE---->
       </td>
     </tr>
   </table>

   <!---Friday Schedule---->

   <table>

     <tr>
       <th></th>
              <th></th>
       <th>Friday, June 24</th>

     </tr>

     <tr class="even">
       <td></td>
       <td>8:00-9:00am</td>
       <td>Registration</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>8:30-10:30am</td>
       <td>Paper Session #3: Moving, Reflecting, Sensing, Knowing | Moderated by Garrett Laroy Johnson</td>
     </tr>

     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>8:30am</td>
               <td>“So… Will You Be Looking at Dance?” Data-led Dance History and the Edges of Movement Computing (L)</td>
               <td>Harmony Bench and Kate Elswit</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Dance historical analysis demands an understanding of “movement data” that includes not just data seen to represent moving bodies in terms of dancing bodies, but also representing movement itself as a way of thinking and knowing, of archiving and transmission. In this paper, we draw on experiences from running Dunham’s Data: Katherine Dunham and Digital Methods for Dance History Inquiry to argue that Dance history can participate in initiatives to build more expansive understandings of measurement, analysis, and representation in movement computing, in particular by addressing kinds of movement that, while not “dancing” per se, are the enabling conditions for dance. We begin with contexts for a data-led dance history, and then elaborate the methods in terms of building, analyzing, and visualizing datasets that focus on transnational, intercorporeal, and intergenerational transmission of dance-based knowledge practices. We collect key findings regarding the capacity of such methodologies to expand the scope of historical movement inquiry, and in turn, to rethink the complexity of embodiment in a broader range of digital analytical contexts. Finally, the conclusion touches on further research, including the sensory potential of visualizing historical dance data in and as movement.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>9:00am</td>
               <td>The I-TEC Design Framework for Kinesthetic Transmission in Online Spaces (L)</td>
               <td>Shannon Cuykendall and Thecla Schiphorst</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Disseminating dance in online spaces provides an opportunity for kinesthetic knowledge to reach broader audiences. However, the transmission of dance in online spaces also primarily relies on visual and aural modalities that cannot fully capture the nuanced physical sensations of a kinesthetic experience. In recent years there has been an influx of interactive online dance resources; yet there is little analysis of how these works effectively translate kinesthetic knowledge to online audiences. We bring together research in dance film and interaction design practices to explore kinesthetic transmission in online spaces and conduct analyses on interactive digital dance resources. Based on our literature review and analyses of these dance resources we propose the I-TEC design framework for kinesthetic transmission. In this framework, Instructional, Translational, Exploratory, and Contextual interactions are brought together to expose the multiple embodiments, perspectives, and translations of kinesthesia.
                 </td>
   </tr>
   </table>
   </td>
   </tr>


             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>9:30am</td>
               <td>Geocultural Precarities in Canonizing Computing Research Involving Dance (L)</td>
               <td>Jessica Rajko</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>

                   </tr>
                   <tr>
                     <td class="abstract">This paper conducts a thorough mapping study and rhetorical analysis of computing research involving dance. Research investigates: 1) who conducts computing research involving dance; 2) how dance is described in computing research publications; and 3) how geoculturally-specific forms of embodied dance knowledge become normalised over time. The study’s publication corpus is extracted from the Association of Computing Machinery (ACM) Digital Library and includes 135 papers returned in a database query using the general keyword search term “dancer.” Results illustrate the geocultural specificity of computing research involving dance and identify rhetorical trends that treat embodied knowledge and methods deriving from western concert dance as the unofficial norm of dance-based movement expertise in computing contexts. The paper’s summaries and discussion consider the effects treating embodied knowledge deriving from western concert dance as universally applicable across geoculturally diverse movement analysis efforts. Here, I particularly address the unintended erasure of embodied knowledge deriving from non-western dance forms and discuss what we might learn in terms of best practices from existing trends in computing research involving non-western dance forms.
                     </td>

   </tr>
   </table>
   </td>
   </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>10:00am</td>
               <td>Feeling movement in live electronic music: An embodied autoethnography (S)</td>
               <td>Mary Mainsbridge</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>

                   </tr>
                   <tr>
                     <td class="abstract">Movement-based interaction relies on the measurement and abstraction of human motion. Yet reducing physical experience to quantitative and visual representations overlooks the inner perceptions and sensations that accompany movement. Rather than treating the body as an object to be observed and analysed, soma-based and autobiographical design approaches instead draw on first-person perspectives of felt experience, providing insight into how the body experiences and interacts with the world. Applying these methods to the musical realm, the paper offers a personal account of working with motion sensing interfaces over the past decade, exploring the underlying qualities and meanings of performance actions in live and recorded contexts. The embodied authethnographic study serves as an opportunity to question normative approaches to technology that favour techno-scientific narratives of external control and surveillance. Alternative values of agency, autonomy, empathy and transparency are explored using first-person methodologies.
                     </td>

                   </tr>
                 </table>
               </td>
             </tr>



             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>10:15am</td>
               <td>Enhancing Film Choreography Through Digital Representation of Camera Movement and Agency (S)</td>
               <td>Jenny Oyallon-Koloski and Michael J. Junokas</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>

                   </tr>
                   <tr>
                     <td class="abstract">Cinematography — aspect ratios, framing, and camera movement, especially — plays an essential role in film choreography. However, the camera’s capacity to expand the aesthetics of dance on film and contribute to the dynamism of figure movement is limited. In commercial (profit-driven) cinematic practice, this limitation is bounded by the physical properties of the equipment, accessibility issues, limited time and financing for stylistic experimentation, and institutional memory loss of cinematic choreography techniques. The result in much of contemporary commercial dance on film is an emphasis on multi-camera coverage, tighter framings, and the use of editing to guide the dynamism of the figure movement in lieu of an emphasis on the relationship between the camera and the body. In this paper, we present theoretical frameworks and motion-capture driven utilities that empower the filmic choreographer beyond traditional, physical limits of the medium. We do so by providing digital representations and interactions using abstracted, artificial systems mimicking the live camera-dancer relationship that prioritize cinematic agency and movement as the principal subject material. This development parallels the growth of previsualization and camera motion control in the field of visual effects, linking us conceptually to existing industrial paradigms. Our initial foray into the expansion of this agency defines a progressive development of filmic choreography, escaping narrow limits of the traditional medium and provoking a more inclusive, accessible, and empowered form of creation through novel access of conventionally unmeasurable capability.
                     </td>

                   </tr>
                 </table>
               </td>
             </tr>



   </table>
   <table>
   </table>
   </td>
   </tr>



     <tr class="even">
       <td></td>
       <td>10:30-11:00am</td>
       <td>Coffee Break</td>
     </tr>

     <tr class="odd">
     <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>11:00am-12:30pm</td>
       <td>Art, Movement, Technology, and Decolonization Practices Panel | Moderated by Kristin Carlson</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th>Panelist</th>
               <th>Institution</th>
             </tr>

             <tr>
               <td>Paula Gaetano-Adi</td>
               <td>Rhode Island School of Design (RISD)</td>
           </tr>
           <tr>
             <td>Yanira Castro</td>
             <td>A Canari Torsi</td>
         </tr>
         <tr>
           <td>Marco Donnarumma</td>
           <td>Independent Artist</td>
       </tr>
       <tr>
         <td>Jessica Rajko</td>
         <td>Wayne State University</td>
     </tr>
   </table>
   <table>

   </table>
</td>
</tr>

     <tr class="even">
       <td></td>
       <td>12:30-1:30pm</td>
       <td>Lunch Break</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>1:30-3:45pm</td>
       <td>Paper Session #4: Movement Recognition and Analysis | Moderated by Elizabeth Jochum</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>

             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>1:30pm</td>
               <td>Movement Analysis and Decomposition with the Continuous Wavelet Transform (L)</td>
               <td>Jules Françoise, Gabriel Meseguer-Brocal and Frédéric Bevilacqua</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Human movements support communication, and can be used to imitate actions or physical phenomenons. Observing gestural imitations of short sounds, we found that such gestures can be categorized by their frequency content. To analyze such movements, we propose an analysis method based on wavelet analysis for clustering or recognizing movement characteristics. Our technique draws upon the continuous wavelet transform to derive a time-frequency representation of movement information. We propose several global descriptors based on statistical descriptors, frequency tracking, or non-negative matrix factorization, that can be used for recognition or clustering to highlight relevant movement qualities. Additionally, we propose a real-time implementation of the continuous wavelet transform based on a set of approximations, that enables its use in interactive applications. Our method is evaluated on a database of gestures co-executed with vocal imitations of recorded sounds.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:00pm</td>
               <td>Machine Art: Exploring Abstract Human Animation Through Machine Learning Methods (L)</td>
               <td>Jamal Knight, Andrew Johnston and Adam Berry</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Visual media and performance art have a symbiotic relationship. They support one another and engage the audience by providing an experience or telling a story. This comparative study explores the accuracy, efficiency, and cost factors of using machine learning based motion capture methods in performance art. There is extensive research in the field of machine learning methods for human pose estimation, but the outputs of such work are rarely used as inputs for performance art. In this paper we present practice-based research project that involves producing animations that match a performer’s movements using machine learning based motion capture methods. We use human poses derived from low-cost video capture as an input into high-resolution abstract forms that accompany and synchronise with dance performances. A single-camera approach is examined and compared to existing methods. We find that compared with existing motion capture methods the machine learning based methods require less setup time, and less equipment is required resulting in considerably lower cost. This research suggests that machine learning has considerable potential to improve the quality of human pose estimation in performance art, visual effects and motion capture, and make it more accessible for arts companies with limited resources.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>2:30pm</td>
               <td>Analysis of variability in hand movement in sign language: development of generative model (L)</td>
               <td>Perrine Chassat, Juhyun Park and Nicolas Brunel</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">Abstract	The analysis of human movement poses a well-known challenge that has already been addressed in various ways and needs to be adapted to the type of movement being considered. The focus here is on the analysis of hand movement in sign language. This study aims to characterize and model the different variations present in the data to develop a realistic generative model of hand movement in sign language. We identify two types of variations that play a key role in characterizing human movement: temporal variations and shape variations, i.e., variations in the speed of movement and the geometry of movement. However, separating these variations or understanding their relationship is a non-trivial task. A well-known model for the relationship between time, speed, and geometry is the 2/3 power-law demonstrated for several human movements, mainly constrained and planar. We find that the generalization of this law to a three-dimensional motion is not sufficient to explain variations in hand movement in sign language. We develop a new statistical modeling framework that is flexible and can respect the geometry of the movement signals. The two different variations are identified using the Frenet-Serret representation and modeled by mean geometry, mean speed, and their nonlinear transformations. The nonlinear variations in time and geometry are analyzed by functional principal component analysis. Then the generative model for the hand movement in sign language is built by imposing a joint probability model on the principal coefficients of these components.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>3:00pm</td>
               <td>Spectral Analysis for Dance Movement Query and Interpolation (S)</td>
               <td>Emily Napier, Gavia Gray and Sageev Oore</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">While it is possible to quantify motion using various transforms or computational techniques, these representations may not be easy to interpret or reconstruct. In this paper, we focus on the problem of visualizing, querying, and manipulating dance components in the form of spectral features. Our first contribution is measuring the similarity of movements in a way that is robust to phase differences while identifying motions with similar pose frequencies. Our second contribution uses the similarity of pose spectra as a metric to drive the interpolation of a motion sequence towards target statistics. We identify the visual impact of these metrics on the characteristics of motion with input from experts in the dance field. These techniques are implemented to explore representations of dance that have the potential to be the basis of more intuitive choreography generation and educational tools for dance artists.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>3:15pm</td>
               <td>Sensor-based Activity Recognition using Deep Learning: A Comparative Study (S)</td>
               <td>Imen Trabelsi, Yacine Bellik and Jules Françoise</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">With the wide availability of inertial sensors in smartphones and connected objects, interest in sensor-based activity recognition has risen.
   Yet, recognizing human actions from inertial data remains a challenging task because of the complexity of human movements and of inter-individual differences in movement execution. Recently, approaches based on deep neural networks have shown success on standardized activity recognition datasets, yet few works investigate systematically how these models generalize to other protocols for data collection. We present a study that evaluates the performance of various deep learning architectures for activity recognition from a single inertial measurement unit, on a recognition task combining data from six publicly available datasets. The best performance on this combined dataset is obtained with an approach combining the continuous wavelet transform and 2D convolutional neural networks.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>3:30pm</td>
               <td>Experimental Creation of Contemporary Dance Works Using a Body-part Motion Synthesis System (S)</td>
               <td>Asako Soga, Bin Umino and Motoko Hirayama</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">We developed a body-part motion synthesis system (BMSS) that synthesizes 3D motion data captured from performances of professional dancers to support the creation of contemporary dance works. To evaluate the usefulness of the system, three professional choreographers created their original dance works experimentally using the BMSS three times, and dancers performed their works in theaters. By analyzing the sequence data created by the BMSS obtained through an interview with the choreographers, we found that the choreographers could discover a variety of uses for the BMSS by becoming proficient in its use. The characteristics of the choreography created by each choreographer were also clarified.
                     </td>
                   </tr>
                 </table>
               </td>
             </tr>

           </table>
           <table>

           </table>
       </td>
     </tr>

     <tr class="even">
       <td></td>
       <td>3:45-4:00pm</td>
       <td>Coffee Break</td>
     </tr>

     <tr class="odd">
       <td id="collapseButton" onclick="collapse(this)">+</td>
       <td>4:00-5:00pm</td>
       <td>Wearables Journal Special Issue Presentations</td>
     </tr>
     <tr id="hidden">
       <td></td>
       <td colspan=3>
           <table>
             <tr>
               <th></th>
               <th>Time</th>
               <th>Title</th>
               <th>Author</th>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:00pm</td>
               <td>Collectively Playable Wearable Music: Practice-situated approaches to participatory relational inquiry</td>
               <td>Seth Thorn</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">We present two practice-situated, participatory investigations using networked wearable sensors to develop movement-responsive, collectively playable musical instruments: a series of four collocated workshops for expert dancers and a distance learning course in which students use wearable technology to enhance embodied learning and feelings of connectedness telematically. We reflect on our exploration of techniques for structuring ensemble improvisations augmented with custom digital musical instruments using aggregate statistical measures, such as variance of participants’ physical orientation as an index of group intention. Participatory design exchanges top-down design methodologies with bottom-up approaches consulting actors’ interests. We follow this approach by evolving our instruments through abductive experiments and trial-and-error tinkering, without strong theories, methods, or models, using elementary signal processing techniques that are meaningfully understood and modified by participants. Our experiences suggest useful scaffolding techniques for educational, transdisciplinary research-creation communities seeking to explore relational ensemble dynamics in telematic and/or physically collocated settings using accessible wearable technologies. Through creative inquiry and participation, technical objects can become bearers of sense and meaning rather than instating mystifying or alienating relations for the participants.</td>
                   </tr>
                 </table>
               </td>
             </tr>
             <tr>
               <td id="collapseButton" onclick="collapse(this)">+</td>
               <td>4:20pm</td>
               <td>Wearables in sociodrama: an embodied mixed-method study of expressiveness in social interactions</td>
               <td>Vilelmini Kalampratsidou, Katerina Elraheb, Philia Issari, Eugenie Georgaca, Dora Skali, Flora Koliouli, Evangelia Karydi, Pandelis Diamantides, Yannis Ioannidis</td>
             </tr>
             <tr id="hidden">
               <td></td>
               <td colspan=2>
                 <table>
                   <tr>
                     <th>Abstract</th>
                   </tr>
                   <tr>
                     <td class="abstract">This mixed-methods study investigates the use of wearable technology in embodied psychology research and
explores the potential of incorporating bio-signals to focus on the bodily impact of the social experience. The
study relies on scientifically established psychological methods of studying social issues, collective relationships
and emotional overloads, such as sociodrama, in combination with participant observation to qualitatively detect
and observe verbal and non-verbal aspects of social behavior. We evaluate the proposed method through a pilot
sociodrama session and reflect on the outcomes. By utilizing an experimental setting that combines video cameras,
microphones and wearable sensors measuring physiological signals -specifically, heart rate-, we explore how
the synchronisation and analysis of the different signals and annotations enables a mixed-method that combines
qualitative and quantitative instruments in studying embodied expressiveness and social interaction.</td>
                   </tr>
                 </table>
               </td>
             </tr>
       

           </table>

           <tr class="even">
             <td></td>
             <td>5:00-5:30pm</td>
             <td>Future MOCO Presentation + Ending Remarks</td>
           </tr>


           <tr class="odd">
             <td></td>
             <td>5:30-7:30pm</td>
             <td>Dinner Break</td>
           </tr>

           <tr class="even">
             <td></td>
             <td>Evening</td>
             <td>Networking & Industry Night at 21c Museum Hotel with AiR Yanira Castro (starting time TBD)</td>
           </tr>


                   </table>




   <!---END OF EDITABLE CODE---->
       </td>
     </tr>
   </table>

     <!--End of program code-->

</div>
  </div>
</div>
</div>

<!-- partial -->
  <script  src="script_program.js"></script>

<script>
function myFunction() {
  var x = document.getElementById("myTopnav");
  if (x.className === "topnav") {
    x.className += " responsive";
  } else {
    x.className = "topnav";
  }
}
</script>
</body>
</html>
